{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96ecd5a7-a3b6-4ff1-81c3-d999d8967010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy.special import softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a7abfa-50a8-49f7-a6a9-4b3a0ea8e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return ''.join(char for char in text if char in (string.ascii_letters + string.digits + string.punctuation + ' '))\n",
    "\n",
    "def remove_tags(text):\n",
    "    return re.sub(r'@\\w+', '', text)\n",
    "\n",
    "def remove_multispace(text):\n",
    "    return re.sub(r'[\\s]+|[\\t]|[.,\"\\']', ' ', text)\n",
    "\n",
    "def preprocess_pipeline(text):\n",
    "\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_multispace(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0914e8f-72bd-4602-a6cf-7694d29821bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "olid = pd.read_csv('data/olid-train-small.csv')\n",
    "olid['text'] = olid['text'].apply(preprocess_pipeline)\n",
    "\n",
    "hasoc = pd.read_csv('data/hasoc-train.csv')\n",
    "hasoc['text'] = hasoc['text'].apply(preprocess_pipeline)\n",
    "\n",
    "test_data = pd.read_csv('data/olid-test.csv')\n",
    "test_data['text'] = test_data['text'].apply(preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef6945c-ab58-412e-9db2-0c09fcb3d725",
   "metadata": {},
   "source": [
    "## training setup from fBert paper:\n",
    "We used a batch-size of eight, Adam optimiser\n",
    "with learning rate 1eâˆ’4, and a linear learning rate\n",
    "warm-up over 10% of the training data. During the\n",
    "training process, the parameters of the transformer\n",
    "model, as well as the parameters of the subsequent\n",
    "layers, were updated. The models were trained\n",
    "using only training data. Furthermore, they were\n",
    "evaluated while training using an evaluation set that\n",
    "had one fifth of the rows in training data. We performed early stopping if the evaluation loss did not\n",
    "improve over ten evaluation steps. All the models\n",
    "were trained for three epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec34eca5-e403-452a-bb54-d229127e52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experimental setup:\n",
    "model_args = ClassificationArgs()\n",
    "model_args.train_batch_size = 8\n",
    "model_args.num_train_epochs = 3\n",
    "model_args.learning_rate = 1e-4\n",
    "model_args.warmup_ratio = 0.1\n",
    "model_args.evaluate_during_training_steps = 20\n",
    "model_args.evaluate_during_training = True\n",
    "model_args.evaluate_during_training_verbose = True\n",
    "model_args.use_early_stopping = True\n",
    "model_args.early_stopping_patience = 30 #this was increased to 20, not 10 as in fbret paper\n",
    "#utilities:\n",
    "\n",
    "model_args.save_eval_checkpoints = False\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.use_multiprocessing=False\n",
    "model_args.use_multiprocessing_for_evaluation=False\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1020dbcd-8a6e-4bd1-9045-71e1c540a151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f952ba47f7479e839fd2c972f267ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8cdf87d7ce4ab4bf3a5fec5ed23ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 3:   0%|          | 0/622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d4143eb5474469824002a3fac0fbb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 3:   0%|          | 0/622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64328d292a8c4f61a2590d1c86f58881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_args.best_model_dir ='outputs/bert_olid'\n",
    "# Load your training and evaluation data as pandas DataFrames\n",
    "eval_fraction = 0.15\n",
    "\n",
    "# Randomly sample data for evaluation set\n",
    "eval_data = olid.sample(frac=eval_fraction, random_state=42)\n",
    "\n",
    "# Data not included in the evaluation set is used for training\n",
    "train_data = olid.drop(eval_data.index)\n",
    "\n",
    "\n",
    "# Create a ClassificationModel\n",
    "bert_model_olid = ClassificationModel(\n",
    "    \"bert\", \"bert-base-uncased\", args=model_args,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "bert_model_olid.train_model(train_data, eval_df=eval_data, output_dir='outputs/bert_olid')\n",
    "\n",
    "# Make predictions on new data\n",
    "#predictions, raw_outputs = model.predict([\"Example text 1\", \"Example text 2\"])\n",
    "\n",
    "# Evaluate the model\n",
    "bert_model_olid_result, bert_model_olid_outputs, bert_model_olid_wrong_predictions = bert_model_olid.eval_model(eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce71edda-bb0d-4171-b7ba-7ba7c9af1dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3de3059fd747d381a509b52ff69b37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea3546fcd524c07aa881b0c5c04ec91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 3:   0%|          | 0/622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980909ea30b84983bf256353a29870a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 3:   0%|          | 0/622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ca1a81054043cc800093da4f9d8545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_args.best_model_dir ='outputs/hatebert_olid'\n",
    "# Load your training and evaluation data as pandas DataFrames\n",
    "eval_fraction = 0.15\n",
    "\n",
    "# Randomly sample data for evaluation set\n",
    "eval_data = olid.sample(frac=eval_fraction, random_state=42)\n",
    "\n",
    "# Data not included in the evaluation set is used for training\n",
    "train_data = olid.drop(eval_data.index)\n",
    "\n",
    "# Create a ClassificationModel\n",
    "hatebert_model_olid = ClassificationModel(\n",
    "    'bert', 'GroNLP/hateBERT', args=model_args\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "hatebert_model_olid.train_model(train_data, eval_df=eval_data, output_dir='output/hatebert_olid')\n",
    "\n",
    "# Make predictions on new data\n",
    "#predictions, raw_outputs = model.predict([\"Example text 1\", \"Example text 2\"])\n",
    "\n",
    "# Evaluate the model\n",
    "hatebert_model_olid_result, hatebert_model_olid_outputs, hatebert_model_olid_wrong_predictions = hatebert_model_olid.eval_model(eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c7d3091-f872-4e99-a314-f2d96c470192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at diptanu/fBERT and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'bert.pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d5356d96c604a73a76360a78ed8c7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b527cb60b6f74c358f83d3a6472acda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 3:   0%|          | 0/622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de363434684b433b9f3e4f84c1db361f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 3:   0%|          | 0/622 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f60a548e074b95a75e779fdc208f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Evaluation:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_args.best_model_dir ='outputs/fbert_olid'\n",
    "# Load your training and evaluation data as pandas DataFrames\n",
    "eval_fraction = 0.15\n",
    "\n",
    "# Randomly sample data for evaluation set\n",
    "eval_data = olid.sample(frac=eval_fraction, random_state=42)\n",
    "\n",
    "# Data not included in the evaluation set is used for training\n",
    "train_data = olid.drop(eval_data.index)\n",
    "\n",
    "# Create a ClassificationModel\n",
    "fbert_model_olid = ClassificationModel(\n",
    "    'bert', 'diptanu/fBERT', args=model_args\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "fbert_model_olid.train_model(train_data, eval_df=eval_data, output_dir='output/fbert_olid')\n",
    "\n",
    "# Make predictions on new data\n",
    "#predictions, raw_outputs = model.predict([\"Example text 1\", \"Example text 2\"])\n",
    "\n",
    "# Evaluate the model\n",
    "fbert_model_olid_result, fbert_model_olid_outputs, fbert_model_olid_wrong_predictions = fbert_model_olid.eval_model(eval_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5ffd6-e579-45cb-951c-606588a86d67",
   "metadata": {},
   "source": [
    "### ensemble methods\n",
    "hard majority voting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45137488-6f78-4cb7-8688-d812c5c9edb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188906117e8a4b73aec8fdef3f8a92b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e92345ddf82477e8ede9f4ed7c716a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b935674e77cc4e588c0332eca54fe618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load individual pre-trained transformer models\n",
    "bert_model_olid = ClassificationModel(\"bert\", \"outputs/bert_olid\")\n",
    "hatebert_model_olid = ClassificationModel(\"bert\", \"outputs/hatebert_olid\")\n",
    "fbert_model_olid = ClassificationModel(\"bert\", \"outputs/fbert_olid\")\n",
    "\n",
    "# Make predictions using individual models\n",
    "predictions1, _ = bert_model_olid.predict(test_data[\"text\"].tolist())\n",
    "predictions2, _ = hatebert_model_olid.predict(test_data[\"text\"].tolist())\n",
    "predictions3, _ = fbert_model_olid.predict(test_data[\"text\"].tolist())\n",
    "\n",
    "# Perform hard majority voting ensemble\n",
    "hard_majority_predictions = np.array([np.argmax(np.bincount(votes)) for votes in zip(predictions1, predictions2, predictions3)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1c213-eb8d-4d70-b31d-e40b67a10a28",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## hard_majority_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e83dea96-2adc-45ad-919d-dd91a4bb55f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608aa226fbae4d019c8ea3a584a2b3c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296a1e4533a84068a0f8602f9fa4d981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e5314fdbd34955b2e18b3e8388d8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions1, bert_model_olid_outputs = bert_model_olid.predict(test_data[\"text\"].tolist())\n",
    "predictions2, hatebert_model_olid_outputs = hatebert_model_olid.predict(test_data[\"text\"].tolist())\n",
    "predictions3, fbert_model_olid_outputs = fbert_model_olid.predict(test_data[\"text\"].tolist())\n",
    "\n",
    "probabilities1 = softmax(bert_model_olid_outputs, axis=1)\n",
    "probabilities2 = softmax(hatebert_model_olid_outputs, axis=1)\n",
    "probabilities3 = softmax(fbert_model_olid_outputs, axis=1)\n",
    "\n",
    "average_probabilities = (probabilities1 + probabilities2 + probabilities3) / 3\n",
    "\n",
    "# Get the class index with the highest average probability for each sample\n",
    "ensemble_predictions = np.argmax(average_probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2f313921-9ed3-4e69-9dee-7ba31e2ab25f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d4d3ce3-ebff-4fe4-9ee8-c53c41a86a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5347011 , 0.4652989 ],\n",
       "       [0.64832554, 0.35167446],\n",
       "       [0.48807834, 0.51192166],\n",
       "       [0.79534822, 0.20465178],\n",
       "       [0.80888279, 0.19111721],\n",
       "       [0.16924852, 0.83075148],\n",
       "       [0.39325228, 0.60674772],\n",
       "       [0.53436552, 0.46563448],\n",
       "       [0.74151631, 0.25848369],\n",
       "       [0.85195355, 0.14804645]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_probabilities[:10]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
